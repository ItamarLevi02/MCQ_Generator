{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import langchain\n",
    "from dotenv import load_dotenv\n",
    "import langchain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "llm = ChatOpenAI(openai_api_key=api_key, model=\"gpt-4o-mini\",temperature=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7f7b82f4a6d0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f7b82f58880>, model_name='gpt-4o-mini', temperature=0.5, openai_api_key='sk-None-zfdG9I9cf9UvPgF93BE9T3BlbkFJsztDSHxVd7bDxW4Qr0yL', openai_proxy='')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_JSON = {\n",
    "    \"1\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "    \"2\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "    \"3\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE=\"\"\"\n",
    "Text:{text}\n",
    "You are an expert MCQ maker. Given the above text, it is your job to \\\n",
    "create a quiz  of {number} multiple choice questions for {subject} students in {tone} tone. \n",
    "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
    "Make sure to format your response like  RESPONSE_JSON below  and use it as a guide. \\\n",
    "Ensure to make {number} MCQs\n",
    "### RESPONSE_JSON\n",
    "{response_json}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\", \"number\", \"subject\", \"tone\", \"response_json\"],\n",
    "    template=TEMPLATE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_chain=LLMChain(llm=llm, prompt=quiz_generation_prompt, output_key=\"quiz\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE2=\"\"\"\n",
    "You are an expert english grammarian and writer. Given a Multiple Choice Quiz for {subject} students.\\\n",
    "You need to evaluate the complexity of the question and give a complete analysis of the quiz. Only use at max 50 words for complexity analysis. \n",
    "if the quiz is not at per with the cognitive and analytical abilities of the students,\\\n",
    "update the quiz questions which needs to be changed and change the tone such that it perfectly fits the student abilities\n",
    "Quiz_MCQs:\n",
    "{quiz}\n",
    "\n",
    "Check from an expert English Writer of the above quiz:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_evaluation_prompt=PromptTemplate(input_variables=[\"subject\", \"quiz\"], template=TEMPLATE2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_chain=LLMChain(llm=llm, prompt=quiz_evaluation_prompt, output_key=\"review\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_evaluate_chain=SequentialChain(chains=[quiz_chain, review_chain], input_variables=[\"text\", \"number\", \"subject\", \"tone\", \"response_json\"],output_variables=[\"quiz\", \"review\"], verbose=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"/Users/itamarlevi/Desktop/MCQ_Generator/data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'r') as file:\n",
    "    TEXT = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Retrieval-Augmented Generation?\n",
      "Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts.\n",
      "\n",
      "Why is Retrieval-Augmented Generation important?\n",
      "LLMs are a key artificial intelligence (AI) technology powering intelligent chatbots and other natural language processing (NLP) applications. The goal is to create bots that can answer user questions in various contexts by cross-referencing authoritative knowledge sources. Unfortunately, the nature of LLM technology introduces unpredictability in LLM responses. Additionally, LLM training data is static and introduces a cut-off date on the knowledge it has.\n",
      "\n",
      "Known challenges of LLMs include:\n",
      "\n",
      "Presenting false information when it does not have the answer.\n",
      "Presenting out-of-date or generic information when the user expects a specific, current response.\n",
      "Creating a response from non-authoritative sources.\n",
      "Creating inaccurate responses due to terminology confusion, wherein different training sources use the same terminology to talk about different things.\n",
      "You can think of the Large Language Model as an over-enthusiastic new employee who refuses to stay informed with current events but will always answer every question with absolute confidence. Unfortunately, such an attitude can negatively impact user trust and is not something you want your chatbots to emulate!\n",
      "\n",
      "RAG is one approach to solving some of these challenges. It redirects the LLM to retrieve relevant information from authoritative, pre-determined knowledge sources. Organizations have greater control over the generated text output, and users gain insights into how the LLM generates the response.\n",
      "\n",
      "What are the benefits of Retrieval-Augmented Generation?\n",
      "RAG technology brings several benefits to an organization's generative AI efforts.\n",
      "\n",
      "Cost-effective implementation\n",
      "Chatbot development typically begins using a foundation model. Foundation models (FMs) are API-accessible LLMs trained on a broad spectrum of generalized and unlabeled data. The computational and financial costs of retraining FMs for organization or domain-specific information are high. RAG is a more cost-effective approach to introducing new data to the LLM. It makes generative artificial intelligence (generative AI) technology more broadly accessible and usable.\n",
      "\n",
      "Current information\n",
      "Even if the original training data sources for an LLM are suitable for your needs, it is challenging to maintain relevancy. RAG allows developers to provide the latest research, statistics, or news to the generative models. They can use RAG to connect the LLM directly to live social media feeds, news sites, or other frequently-updated information sources. The LLM can then provide the latest information to the users.\n",
      "\n",
      "Enhanced user trust\n",
      "RAG allows the LLM to present accurate information with source attribution. The output can include citations or references to sources. Users can also look up source documents themselves if they require further clarification or more detail. This can increase trust and confidence in your generative AI solution.\n",
      "\n",
      "More developer control\n",
      "With RAG, developers can test and improve their chat applications more efficiently. They can control and change the LLM's information sources to adapt to changing requirements or cross-functional usage. Developers can also restrict sensitive information retrieval to different authorization levels and ensure the LLM generates appropriate responses. In addition, they can also troubleshoot and make fixes if the LLM references incorrect information sources for specific questions. Organizations can implement generative AI technology more confidently for a broader range of applications.\n",
      "\n",
      "How does Retrieval-Augmented Generation work?\n",
      "Without RAG, the LLM takes the user input and creates a response based on information it was trained on—or what it already knows. With RAG, an information retrieval component is introduced that utilizes the user input to first pull information from a new data source. The user query and the relevant information are both given to the LLM. The LLM uses the new knowledge and its training data to create better responses. The following sections provide an overview of the process.\n",
      "\n",
      "Create external data\n",
      "The new data outside of the LLM's original training data set is called external data. It can come from multiple data sources, such as a APIs, databases, or document repositories. The data may exist in various formats like files, database records, or long-form text. Another AI technique, called embedding language models, converts data into numerical representations and stores it in a vector database. This process creates a knowledge library that the generative AI models can understand.\n",
      "\n",
      "Retrieve relevant information\n",
      "The next step is to perform a relevancy search. The user query is converted to a vector representation and matched with the vector databases. For example, consider a smart chatbot that can answer human resource questions for an organization. If an employee searches, \"How much annual leave do I have?\" the system will retrieve annual leave policy documents alongside the individual employee's past leave record. These specific documents will be returned because they are highly-relevant to what the employee has input. The relevancy was calculated and established using mathematical vector calculations and representations.\n",
      "\n",
      "Augment the LLM prompt\n",
      "Next, the RAG model augments the user input (or prompts) by adding the relevant retrieved data in context. This step uses prompt engineering techniques to communicate effectively with the LLM. The augmented prompt allows the large language models to generate an accurate answer to user queries.\n",
      "\n",
      "Update external data\n",
      "The next question may be—what if the external data becomes stale? To maintain current information for retrieval, asynchronously update the documents and update embedding representation of the documents. You can do this through automated real-time processes or periodic batch processing. This is a common challenge in data analytics—different data-science approaches to change management can be used.\n"
     ]
    }
   ],
   "source": [
    "print(TEXT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(RESPONSE_JSON)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUMBER=5 \n",
    "SUBJECT=\"AI\"\n",
    "TONE=\"simple\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Text:What is Retrieval-Augmented Generation?\n",
      "Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts.\n",
      "\n",
      "Why is Retrieval-Augmented Generation important?\n",
      "LLMs are a key artificial intelligence (AI) technology powering intelligent chatbots and other natural language processing (NLP) applications. The goal is to create bots that can answer user questions in various contexts by cross-referencing authoritative knowledge sources. Unfortunately, the nature of LLM technology introduces unpredictability in LLM responses. Additionally, LLM training data is static and introduces a cut-off date on the knowledge it has.\n",
      "\n",
      "Known challenges of LLMs include:\n",
      "\n",
      "Presenting false information when it does not have the answer.\n",
      "Presenting out-of-date or generic information when the user expects a specific, current response.\n",
      "Creating a response from non-authoritative sources.\n",
      "Creating inaccurate responses due to terminology confusion, wherein different training sources use the same terminology to talk about different things.\n",
      "You can think of the Large Language Model as an over-enthusiastic new employee who refuses to stay informed with current events but will always answer every question with absolute confidence. Unfortunately, such an attitude can negatively impact user trust and is not something you want your chatbots to emulate!\n",
      "\n",
      "RAG is one approach to solving some of these challenges. It redirects the LLM to retrieve relevant information from authoritative, pre-determined knowledge sources. Organizations have greater control over the generated text output, and users gain insights into how the LLM generates the response.\n",
      "\n",
      "What are the benefits of Retrieval-Augmented Generation?\n",
      "RAG technology brings several benefits to an organization's generative AI efforts.\n",
      "\n",
      "Cost-effective implementation\n",
      "Chatbot development typically begins using a foundation model. Foundation models (FMs) are API-accessible LLMs trained on a broad spectrum of generalized and unlabeled data. The computational and financial costs of retraining FMs for organization or domain-specific information are high. RAG is a more cost-effective approach to introducing new data to the LLM. It makes generative artificial intelligence (generative AI) technology more broadly accessible and usable.\n",
      "\n",
      "Current information\n",
      "Even if the original training data sources for an LLM are suitable for your needs, it is challenging to maintain relevancy. RAG allows developers to provide the latest research, statistics, or news to the generative models. They can use RAG to connect the LLM directly to live social media feeds, news sites, or other frequently-updated information sources. The LLM can then provide the latest information to the users.\n",
      "\n",
      "Enhanced user trust\n",
      "RAG allows the LLM to present accurate information with source attribution. The output can include citations or references to sources. Users can also look up source documents themselves if they require further clarification or more detail. This can increase trust and confidence in your generative AI solution.\n",
      "\n",
      "More developer control\n",
      "With RAG, developers can test and improve their chat applications more efficiently. They can control and change the LLM's information sources to adapt to changing requirements or cross-functional usage. Developers can also restrict sensitive information retrieval to different authorization levels and ensure the LLM generates appropriate responses. In addition, they can also troubleshoot and make fixes if the LLM references incorrect information sources for specific questions. Organizations can implement generative AI technology more confidently for a broader range of applications.\n",
      "\n",
      "How does Retrieval-Augmented Generation work?\n",
      "Without RAG, the LLM takes the user input and creates a response based on information it was trained on—or what it already knows. With RAG, an information retrieval component is introduced that utilizes the user input to first pull information from a new data source. The user query and the relevant information are both given to the LLM. The LLM uses the new knowledge and its training data to create better responses. The following sections provide an overview of the process.\n",
      "\n",
      "Create external data\n",
      "The new data outside of the LLM's original training data set is called external data. It can come from multiple data sources, such as a APIs, databases, or document repositories. The data may exist in various formats like files, database records, or long-form text. Another AI technique, called embedding language models, converts data into numerical representations and stores it in a vector database. This process creates a knowledge library that the generative AI models can understand.\n",
      "\n",
      "Retrieve relevant information\n",
      "The next step is to perform a relevancy search. The user query is converted to a vector representation and matched with the vector databases. For example, consider a smart chatbot that can answer human resource questions for an organization. If an employee searches, \"How much annual leave do I have?\" the system will retrieve annual leave policy documents alongside the individual employee's past leave record. These specific documents will be returned because they are highly-relevant to what the employee has input. The relevancy was calculated and established using mathematical vector calculations and representations.\n",
      "\n",
      "Augment the LLM prompt\n",
      "Next, the RAG model augments the user input (or prompts) by adding the relevant retrieved data in context. This step uses prompt engineering techniques to communicate effectively with the LLM. The augmented prompt allows the large language models to generate an accurate answer to user queries.\n",
      "\n",
      "Update external data\n",
      "The next question may be—what if the external data becomes stale? To maintain current information for retrieval, asynchronously update the documents and update embedding representation of the documents. You can do this through automated real-time processes or periodic batch processing. This is a common challenge in data analytics—different data-science approaches to change management can be used.\n",
      "You are an expert MCQ maker. Given the above text, it is your job to create a quiz  of 5 multiple choice questions for AI students in simple tone. \n",
      "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
      "Make sure to format your response like  RESPONSE_JSON below  and use it as a guide. Ensure to make 5 MCQs\n",
      "### RESPONSE_JSON\n",
      "{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Text:What is Retrieval-Augmented Generation?\n",
      "Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts.\n",
      "\n",
      "Why is Retrieval-Augmented Generation important?\n",
      "LLMs are a key artificial intelligence (AI) technology powering intelligent chatbots and other natural language processing (NLP) applications. The goal is to create bots that can answer user questions in various contexts by cross-referencing authoritative knowledge sources. Unfortunately, the nature of LLM technology introduces unpredictability in LLM responses. Additionally, LLM training data is static and introduces a cut-off date on the knowledge it has.\n",
      "\n",
      "Known challenges of LLMs include:\n",
      "\n",
      "Presenting false information when it does not have the answer.\n",
      "Presenting out-of-date or generic information when the user expects a specific, current response.\n",
      "Creating a response from non-authoritative sources.\n",
      "Creating inaccurate responses due to terminology confusion, wherein different training sources use the same terminology to talk about different things.\n",
      "You can think of the Large Language Model as an over-enthusiastic new employee who refuses to stay informed with current events but will always answer every question with absolute confidence. Unfortunately, such an attitude can negatively impact user trust and is not something you want your chatbots to emulate!\n",
      "\n",
      "RAG is one approach to solving some of these challenges. It redirects the LLM to retrieve relevant information from authoritative, pre-determined knowledge sources. Organizations have greater control over the generated text output, and users gain insights into how the LLM generates the response.\n",
      "\n",
      "What are the benefits of Retrieval-Augmented Generation?\n",
      "RAG technology brings several benefits to an organization's generative AI efforts.\n",
      "\n",
      "Cost-effective implementation\n",
      "Chatbot development typically begins using a foundation model. Foundation models (FMs) are API-accessible LLMs trained on a broad spectrum of generalized and unlabeled data. The computational and financial costs of retraining FMs for organization or domain-specific information are high. RAG is a more cost-effective approach to introducing new data to the LLM. It makes generative artificial intelligence (generative AI) technology more broadly accessible and usable.\n",
      "\n",
      "Current information\n",
      "Even if the original training data sources for an LLM are suitable for your needs, it is challenging to maintain relevancy. RAG allows developers to provide the latest research, statistics, or news to the generative models. They can use RAG to connect the LLM directly to live social media feeds, news sites, or other frequently-updated information sources. The LLM can then provide the latest information to the users.\n",
      "\n",
      "Enhanced user trust\n",
      "RAG allows the LLM to present accurate information with source attribution. The output can include citations or references to sources. Users can also look up source documents themselves if they require further clarification or more detail. This can increase trust and confidence in your generative AI solution.\n",
      "\n",
      "More developer control\n",
      "With RAG, developers can test and improve their chat applications more efficiently. They can control and change the LLM's information sources to adapt to changing requirements or cross-functional usage. Developers can also restrict sensitive information retrieval to different authorization levels and ensure the LLM generates appropriate responses. In addition, they can also troubleshoot and make fixes if the LLM references incorrect information sources for specific questions. Organizations can implement generative AI technology more confidently for a broader range of applications.\n",
      "\n",
      "How does Retrieval-Augmented Generation work?\n",
      "Without RAG, the LLM takes the user input and creates a response based on information it was trained on—or what it already knows. With RAG, an information retrieval component is introduced that utilizes the user input to first pull information from a new data source. The user query and the relevant information are both given to the LLM. The LLM uses the new knowledge and its training data to create better responses. The following sections provide an overview of the process.\n",
      "\n",
      "Create external data\n",
      "The new data outside of the LLM's original training data set is called external data. It can come from multiple data sources, such as a APIs, databases, or document repositories. The data may exist in various formats like files, database records, or long-form text. Another AI technique, called embedding language models, converts data into numerical representations and stores it in a vector database. This process creates a knowledge library that the generative AI models can understand.\n",
      "\n",
      "Retrieve relevant information\n",
      "The next step is to perform a relevancy search. The user query is converted to a vector representation and matched with the vector databases. For example, consider a smart chatbot that can answer human resource questions for an organization. If an employee searches, \"How much annual leave do I have?\" the system will retrieve annual leave policy documents alongside the individual employee's past leave record. These specific documents will be returned because they are highly-relevant to what the employee has input. The relevancy was calculated and established using mathematical vector calculations and representations.\n",
      "\n",
      "Augment the LLM prompt\n",
      "Next, the RAG model augments the user input (or prompts) by adding the relevant retrieved data in context. This step uses prompt engineering techniques to communicate effectively with the LLM. The augmented prompt allows the large language models to generate an accurate answer to user queries.\n",
      "\n",
      "Update external data\n",
      "The next question may be—what if the external data becomes stale? To maintain current information for retrieval, asynchronously update the documents and update embedding representation of the documents. You can do this through automated real-time processes or periodic batch processing. This is a common challenge in data analytics—different data-science approaches to change management can be used.\n",
      "You are an expert MCQ maker. Given the above text, it is your job to create a quiz  of 5 multiple choice questions for AI students in simple tone. \n",
      "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
      "Make sure to format your response like  RESPONSE_JSON below  and use it as a guide. Ensure to make 5 MCQs\n",
      "### RESPONSE_JSON\n",
      "{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    response=generate_evaluate_chain(\n",
    "        {\n",
    "            \"text\": TEXT,\n",
    "            \"number\": NUMBER,\n",
    "            \"subject\":SUBJECT,\n",
    "            \"tone\": TONE,\n",
    "            \"response_json\": json.dumps(RESPONSE_JSON)\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens:3726\n",
      "Prompt Tokens:2928\n",
      "Completion Tokens:798\n",
      "Total Cost:0.0009179999999999999\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Tokens:{cb.total_tokens}\")\n",
    "print(f\"Prompt Tokens:{cb.prompt_tokens}\")\n",
    "print(f\"Completion Tokens:{cb.completion_tokens}\")\n",
    "print(f\"Total Cost:{cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'What is Retrieval-Augmented Generation?\\nRetrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization\\'s internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts.\\n\\nWhy is Retrieval-Augmented Generation important?\\nLLMs are a key artificial intelligence (AI) technology powering intelligent chatbots and other natural language processing (NLP) applications. The goal is to create bots that can answer user questions in various contexts by cross-referencing authoritative knowledge sources. Unfortunately, the nature of LLM technology introduces unpredictability in LLM responses. Additionally, LLM training data is static and introduces a cut-off date on the knowledge it has.\\n\\nKnown challenges of LLMs include:\\n\\nPresenting false information when it does not have the answer.\\nPresenting out-of-date or generic information when the user expects a specific, current response.\\nCreating a response from non-authoritative sources.\\nCreating inaccurate responses due to terminology confusion, wherein different training sources use the same terminology to talk about different things.\\nYou can think of the Large Language Model as an over-enthusiastic new employee who refuses to stay informed with current events but will always answer every question with absolute confidence. Unfortunately, such an attitude can negatively impact user trust and is not something you want your chatbots to emulate!\\n\\nRAG is one approach to solving some of these challenges. It redirects the LLM to retrieve relevant information from authoritative, pre-determined knowledge sources. Organizations have greater control over the generated text output, and users gain insights into how the LLM generates the response.\\n\\nWhat are the benefits of Retrieval-Augmented Generation?\\nRAG technology brings several benefits to an organization\\'s generative AI efforts.\\n\\nCost-effective implementation\\nChatbot development typically begins using a foundation model. Foundation models (FMs) are API-accessible LLMs trained on a broad spectrum of generalized and unlabeled data. The computational and financial costs of retraining FMs for organization or domain-specific information are high. RAG is a more cost-effective approach to introducing new data to the LLM. It makes generative artificial intelligence (generative AI) technology more broadly accessible and usable.\\n\\nCurrent information\\nEven if the original training data sources for an LLM are suitable for your needs, it is challenging to maintain relevancy. RAG allows developers to provide the latest research, statistics, or news to the generative models. They can use RAG to connect the LLM directly to live social media feeds, news sites, or other frequently-updated information sources. The LLM can then provide the latest information to the users.\\n\\nEnhanced user trust\\nRAG allows the LLM to present accurate information with source attribution. The output can include citations or references to sources. Users can also look up source documents themselves if they require further clarification or more detail. This can increase trust and confidence in your generative AI solution.\\n\\nMore developer control\\nWith RAG, developers can test and improve their chat applications more efficiently. They can control and change the LLM\\'s information sources to adapt to changing requirements or cross-functional usage. Developers can also restrict sensitive information retrieval to different authorization levels and ensure the LLM generates appropriate responses. In addition, they can also troubleshoot and make fixes if the LLM references incorrect information sources for specific questions. Organizations can implement generative AI technology more confidently for a broader range of applications.\\n\\nHow does Retrieval-Augmented Generation work?\\nWithout RAG, the LLM takes the user input and creates a response based on information it was trained on—or what it already knows. With RAG, an information retrieval component is introduced that utilizes the user input to first pull information from a new data source. The user query and the relevant information are both given to the LLM. The LLM uses the new knowledge and its training data to create better responses. The following sections provide an overview of the process.\\n\\nCreate external data\\nThe new data outside of the LLM\\'s original training data set is called external data. It can come from multiple data sources, such as a APIs, databases, or document repositories. The data may exist in various formats like files, database records, or long-form text. Another AI technique, called embedding language models, converts data into numerical representations and stores it in a vector database. This process creates a knowledge library that the generative AI models can understand.\\n\\nRetrieve relevant information\\nThe next step is to perform a relevancy search. The user query is converted to a vector representation and matched with the vector databases. For example, consider a smart chatbot that can answer human resource questions for an organization. If an employee searches, \"How much annual leave do I have?\" the system will retrieve annual leave policy documents alongside the individual employee\\'s past leave record. These specific documents will be returned because they are highly-relevant to what the employee has input. The relevancy was calculated and established using mathematical vector calculations and representations.\\n\\nAugment the LLM prompt\\nNext, the RAG model augments the user input (or prompts) by adding the relevant retrieved data in context. This step uses prompt engineering techniques to communicate effectively with the LLM. The augmented prompt allows the large language models to generate an accurate answer to user queries.\\n\\nUpdate external data\\nThe next question may be—what if the external data becomes stale? To maintain current information for retrieval, asynchronously update the documents and update embedding representation of the documents. You can do this through automated real-time processes or periodic batch processing. This is a common challenge in data analytics—different data-science approaches to change management can be used.',\n",
       " 'number': 5,\n",
       " 'subject': 'AI',\n",
       " 'tone': 'simple',\n",
       " 'response_json': '{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}',\n",
       " 'quiz': '{\"1\": {\"mcq\": \"What does Retrieval-Augmented Generation (RAG) aim to improve in large language models?\", \"options\": {\"a\": \"The speed of response\", \"b\": \"The accuracy and relevance of responses\", \"c\": \"The size of the model\", \"d\": \"The cost of training\"}, \"correct\": \"b\"}, \"2\": {\"mcq\": \"What is the main challenge faced by large language models (LLMs)?\", \"options\": {\"a\": \"They are too small\", \"b\": \"They always provide accurate information\", \"c\": \"They may present false or outdated information\", \"d\": \"They cannot generate original content\"}, \"correct\": \"c\"}, \"3\": {\"mcq\": \"How does RAG enhance user trust?\", \"options\": {\"a\": \"By generating responses quickly\", \"b\": \"By providing source citations and references\", \"c\": \"By using only internal data\", \"d\": \"By avoiding user queries\"}, \"correct\": \"b\"}, \"4\": {\"mcq\": \"What is the process of creating external data in RAG?\", \"options\": {\"a\": \"Retraining the LLM\", \"b\": \"Converting data into numerical representations\", \"c\": \"Only using text data\", \"d\": \"Ignoring external sources\"}, \"correct\": \"b\"}, \"5\": {\"mcq\": \"What happens after the relevant information is retrieved in RAG?\", \"options\": {\"a\": \"It is discarded\", \"b\": \"It is used to augment the LLM prompt\", \"c\": \"It is sent to a different model\", \"d\": \"It is stored permanently\"}, \"correct\": \"b\"}}',\n",
       " 'review': '```JSON\\n{\\n  \"1\": {\\n    \"mcq\": \"What does Retrieval-Augmented Generation (RAG) primarily enhance in large language models?\",\\n    \"options\": {\\n      \"a\": \"The speed of response generation\",\\n      \"b\": \"The accuracy and relevance of responses\",\\n      \"c\": \"The size of the training dataset\",\\n      \"d\": \"The number of parameters in the model\"\\n    },\\n    \"correct\": \"b\"\\n  },\\n  \"2\": {\\n    \"mcq\": \"What is one of the main challenges faced by large language models (LLMs)?\",\\n    \"options\": {\\n      \"a\": \"They can only answer historical questions.\",\\n      \"b\": \"They often present false or outdated information.\",\\n      \"c\": \"They require constant retraining.\",\\n      \"d\": \"They cannot process text.\"\\n    },\\n    \"correct\": \"b\"\\n  },\\n  \"3\": {\\n    \"mcq\": \"How does RAG improve user trust in LLM outputs?\",\\n    \"options\": {\\n      \"a\": \"By generating responses faster\",\\n      \"b\": \"By providing citations and references to sources\",\\n      \"c\": \"By using more complex algorithms\",\\n      \"d\": \"By limiting the number of questions users can ask\"\\n    },\\n    \"correct\": \"b\"\\n  },\\n  \"4\": {\\n    \"mcq\": \"What is the first step in the RAG process?\",\\n    \"options\": {\\n      \"a\": \"Retrieve relevant information\",\\n      \"b\": \"Create external data\",\\n      \"c\": \"Augment the LLM prompt\",\\n      \"d\": \"Update external data\"\\n    },\\n    \"correct\": \"b\"\\n  },\\n  \"5\": {\\n    \"mcq\": \"Why is RAG considered a cost-effective approach for organizations?\",\\n    \"options\": {\\n      \"a\": \"It eliminates the need for any data.\",\\n      \"b\": \"It avoids the high costs of retraining foundation models.\",\\n      \"c\": \"It requires fewer developers to implement.\",\\n      \"d\": \"It can be done without any external data sources.\"\\n    },\\n    \"correct\": \"b\"\\n  }\\n}\\n```'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_data = response.get(\"quiz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz=json.loads(quiz_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'mcq': 'What does Retrieval-Augmented Generation (RAG) aim to improve in large language models?',\n",
       "  'options': {'a': 'The speed of response',\n",
       "   'b': 'The accuracy and relevance of responses',\n",
       "   'c': 'The size of the model',\n",
       "   'd': 'The cost of training'},\n",
       "  'correct': 'b'},\n",
       " '2': {'mcq': 'What is the main challenge faced by large language models (LLMs)?',\n",
       "  'options': {'a': 'They are too small',\n",
       "   'b': 'They always provide accurate information',\n",
       "   'c': 'They may present false or outdated information',\n",
       "   'd': 'They cannot generate original content'},\n",
       "  'correct': 'c'},\n",
       " '3': {'mcq': 'How does RAG enhance user trust?',\n",
       "  'options': {'a': 'By generating responses quickly',\n",
       "   'b': 'By providing source citations and references',\n",
       "   'c': 'By using only internal data',\n",
       "   'd': 'By avoiding user queries'},\n",
       "  'correct': 'b'},\n",
       " '4': {'mcq': 'What is the process of creating external data in RAG?',\n",
       "  'options': {'a': 'Retraining the LLM',\n",
       "   'b': 'Converting data into numerical representations',\n",
       "   'c': 'Only using text data',\n",
       "   'd': 'Ignoring external sources'},\n",
       "  'correct': 'b'},\n",
       " '5': {'mcq': 'What happens after the relevant information is retrieved in RAG?',\n",
       "  'options': {'a': 'It is discarded',\n",
       "   'b': 'It is used to augment the LLM prompt',\n",
       "   'c': 'It is sent to a different model',\n",
       "   'd': 'It is stored permanently'},\n",
       "  'correct': 'b'}}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_table_data = []\n",
    "for key, value in quiz.items():\n",
    "    mcq = value[\"mcq\"]\n",
    "    options = \" | \".join(\n",
    "        [\n",
    "            f\"{option}: {option_value}\"\n",
    "            for option, option_value in value[\"options\"].items()\n",
    "            ]\n",
    "        )\n",
    "    correct = value[\"correct\"]\n",
    "    quiz_table_data.append({\"MCQ\": mcq, \"Choices\": options, \"Correct\": correct})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'MCQ': 'What does Retrieval-Augmented Generation (RAG) aim to improve in large language models?',\n",
       "  'Choices': 'a: The speed of response | b: The accuracy and relevance of responses | c: The size of the model | d: The cost of training',\n",
       "  'Correct': 'b'},\n",
       " {'MCQ': 'What is the main challenge faced by large language models (LLMs)?',\n",
       "  'Choices': 'a: They are too small | b: They always provide accurate information | c: They may present false or outdated information | d: They cannot generate original content',\n",
       "  'Correct': 'c'},\n",
       " {'MCQ': 'How does RAG enhance user trust?',\n",
       "  'Choices': 'a: By generating responses quickly | b: By providing source citations and references | c: By using only internal data | d: By avoiding user queries',\n",
       "  'Correct': 'b'},\n",
       " {'MCQ': 'What is the process of creating external data in RAG?',\n",
       "  'Choices': 'a: Retraining the LLM | b: Converting data into numerical representations | c: Only using text data | d: Ignoring external sources',\n",
       "  'Correct': 'b'},\n",
       " {'MCQ': 'What happens after the relevant information is retrieved in RAG?',\n",
       "  'Choices': 'a: It is discarded | b: It is used to augment the LLM prompt | c: It is sent to a different model | d: It is stored permanently',\n",
       "  'Correct': 'b'}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz_table_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz=pd.DataFrame(quiz_table_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MCQ</th>\n",
       "      <th>Choices</th>\n",
       "      <th>Correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What does Retrieval-Augmented Generation (RAG)...</td>\n",
       "      <td>a: The speed of response | b: The accuracy and...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the main challenge faced by large lang...</td>\n",
       "      <td>a: They are too small | b: They always provide...</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does RAG enhance user trust?</td>\n",
       "      <td>a: By generating responses quickly | b: By pro...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the process of creating external data ...</td>\n",
       "      <td>a: Retraining the LLM | b: Converting data int...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What happens after the relevant information is...</td>\n",
       "      <td>a: It is discarded | b: It is used to augment ...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 MCQ  \\\n",
       "0  What does Retrieval-Augmented Generation (RAG)...   \n",
       "1  What is the main challenge faced by large lang...   \n",
       "2                   How does RAG enhance user trust?   \n",
       "3  What is the process of creating external data ...   \n",
       "4  What happens after the relevant information is...   \n",
       "\n",
       "                                             Choices Correct  \n",
       "0  a: The speed of response | b: The accuracy and...       b  \n",
       "1  a: They are too small | b: They always provide...       c  \n",
       "2  a: By generating responses quickly | b: By pro...       b  \n",
       "3  a: Retraining the LLM | b: Converting data int...       b  \n",
       "4  a: It is discarded | b: It is used to augment ...       b  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz.to_csv(\"machinelearning.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'08_02_2024_15_19_27'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "datetime.now().strftime('%m_%d_%Y_%H_%M_%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
